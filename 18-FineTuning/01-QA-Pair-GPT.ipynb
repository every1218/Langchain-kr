{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Pair 를 생성할 PDF 를 로드합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pdfminer.psexceptions'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m partition_pdf\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_pdf_elements\u001b[39m(filepath):\n\u001b[32m      5\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m    PDF 파일에서 이미지, 테이블, 그리고 텍스트 조각을 추출합니다.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m    path: 이미지(.jpg)를 저장할 파일 경로\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m    fname: 파일 이름\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DOHYEON\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr--LeusF5K-py3.11\\Lib\\site-packages\\unstructured\\partition\\pdf.py:70\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdf_image\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mform_extraction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_form_extraction\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdf_image\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdf_image_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     66\u001b[39m     check_element_types_to_extract,\n\u001b[32m     67\u001b[39m     convert_pdf_to_images,\n\u001b[32m     68\u001b[39m     save_elements,\n\u001b[32m     69\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdf_image\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdfminer_processing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     71\u001b[39m     check_annotations_within_element,\n\u001b[32m     72\u001b[39m     clean_pdfminer_inner_elements,\n\u001b[32m     73\u001b[39m     get_links_in_element,\n\u001b[32m     74\u001b[39m     get_uris,\n\u001b[32m     75\u001b[39m     get_words_from_obj,\n\u001b[32m     76\u001b[39m     map_bbox_and_index,\n\u001b[32m     77\u001b[39m     merge_inferred_with_extracted_layout,\n\u001b[32m     78\u001b[39m )\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdf_image\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdfminer_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     80\u001b[39m     PDFMinerConfig,\n\u001b[32m     81\u001b[39m     open_pdfminer_pages_generator,\n\u001b[32m     82\u001b[39m     rect_to_bbox,\n\u001b[32m     83\u001b[39m )\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstrategies\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m determine_pdf_or_image_strategy, validate_strategy\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DOHYEON\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr--LeusF5K-py3.11\\Lib\\site-packages\\unstructured\\partition\\pdf_image\\pdfminer_processing.py:17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocuments\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01melements\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CoordinatesMetadata, ElementType\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdf_image\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdf_image_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m remove_control_characters\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdf_image\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdfminer_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     18\u001b[39m     PDFMinerConfig,\n\u001b[32m     19\u001b[39m     extract_image_objects,\n\u001b[32m     20\u001b[39m     extract_text_objects,\n\u001b[32m     21\u001b[39m     open_pdfminer_pages_generator,\n\u001b[32m     22\u001b[39m     rect_to_bbox,\n\u001b[32m     23\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m env_config\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SORT_MODE_BASIC, Source\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DOHYEON\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr--LeusF5K-py3.11\\Lib\\site-packages\\unstructured\\partition\\pdf_image\\pdfminer_utils.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpdfminer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdfinterp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PDFPageInterpreter, PDFResourceManager\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpdfminer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdfpage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PDFPage\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpdfminer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpsexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PSSyntaxError\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logger\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pdfminer.psexceptions'"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "\n",
    "def extract_pdf_elements(filepath):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 이미지, 테이블, 그리고 텍스트 조각을 추출합니다.\n",
    "    path: 이미지(.jpg)를 저장할 파일 경로\n",
    "    fname: 파일 이름\n",
    "    \"\"\"\n",
    "    return partition_pdf(\n",
    "        filename=filepath,\n",
    "        extract_images_in_pdf=False,  # PDF 내 이미지 추출 활성화\n",
    "        infer_table_structure=False,  # 테이블 구조 추론 활성화\n",
    "        chunking_strategy=\"by_title\",  # 제목별로 텍스트 조각화\n",
    "        max_characters=4000,  # 최대 문자 수\n",
    "        new_after_n_chars=3800,  # 이 문자 수 이후에 새로운 조각 생성\n",
    "        combine_text_under_n_chars=2000,  # 이 문자 수 이하의 텍스트는 결합\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 파일 로드\n",
    "elements = extract_pdf_elements(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 로드한 TEXT 청크 수\n",
    "len(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위험 식별과 완화에 필요한 조치를 포함\n",
      "\n",
      "주요 7개국(G7)은 미국, 일본, 독일, 영국, 프랑스, 이탈리아, 캐나다를 의미\n",
      "\n",
      "** 5월 정상회의에는 한국, 호주, 베트남 등을 포함한 8개국이 초청을 받았으나, AI 국제 행동강령에는 우선 G7 국가만 포함하여 채택\n",
      "\n",
      "n G7은 행동강령을 통해 아래의 조치를 제시했으며, 빠르게 발전하는 기술에 대응할 수 있도록\n",
      "\n",
      "이해관계자 협의를 통해 필요에 따라 개정할 예정\n",
      "\n",
      "첨단 AI 시스템의 개발 과정에서 AI 수명주기 전반에 걸쳐 위험을 평가 및 완화하는 조치를 채택하고,\n",
      "\n",
      "첨단 AI 시스템의 출시와 배포 이후 취약점과 오용 사고, 오용 유형을 파악해 완화\n",
      "\n",
      "첨단 AI 시스템의 성능과 한계를 공개하고 적절하거나 부적절한 사용영역을 알리는 방법으로 투명성을\n",
      "\n",
      "보장하고 책임성을 강화\n",
      "\n",
      "산업계, 정부, 시민사회, 학계를 포함해 첨단 AI 시스템을 개발하는 조직 간 정보공유와 사고 발생 시\n",
      "\n",
      "신고를 위해 협력하고, 위험 기반 접근방식을 토대로 개인정보보호 정책과 위험 완화 조치를 포함하는\n",
      "\n",
      "AI 거버넌스와 위험 관리 정책을 마련\n",
      "\n",
      "AI 수명주기 전반에 걸쳐 물리보안, 사이버보안, 내부자 위협 보안을 포함한 강력한 보안 통제 구현\n",
      "\n",
      "사용자가 AI 생성 콘텐츠를 식별할 수 있도록 워터마크를 비롯하여 기술적으로 가능한 기법으로\n",
      "\n",
      "신뢰할 수 있는 콘텐츠 인증과 출처 확인 메커니즘을 개발 및 구축\n",
      "\n",
      "사회적 위험과 안전·보안 문제를 완화하는 연구와 효과적인 완화 대책에 우선 투자하고, 기후 위기\n",
      "\n",
      "대응, 세계 보건과 교육 등 세계적 난제 해결을 위한 첨단 AI 시스템을 우선 개발\n",
      "\n",
      "국제 기술 표준의 개발 및 채택을 가속화하고, 개인정보와 지식재산권 보호를 위해 데이터 입력과 수집\n",
      "\n",
      "시 적절한 보호 장치 구현\n",
      "\n",
      "☞ 출처: G7, Hiroshima Process International Code of Conduct for Advanced AI Systems, 2023.10.30.\n",
      "\n",
      "2\n",
      "\n",
      "1. 정책/법제\n",
      "\n",
      "2. 기업/산업\n",
      "\n",
      "3. 기술/연구\n",
      "\n",
      "4. 인력/교육\n",
      "\n",
      "영국 AI 안전성 정상회의에 참가한 28개국, AI 위험에 공동 대응 선언\n",
      "\n",
      "KEY Contents\n",
      "\n",
      "n 영국 블레츨리 파크에서 개최된 AI 안전성 정상회의에 참가한 28개국들이 AI 안전 보장을\n",
      "\n",
      "위한 협력 방안을 담은 블레츨리 선언을 발표\n",
      "\n",
      "n 첨단 AI를 개발하는 국가와 기업들은 AI 시스템에 대한 안전 테스트 계획에 합의했으며,\n",
      "\n",
      "영국의 AI 안전 연구소가 전 세계 국가와 협력해 테스트를 주도할 예정\n",
      "\n",
      "£ AI 안전성 정상회의 참가국들, 블레츨리 선언 통해 AI 안전 보장을 위한 협력에 합의\n",
      "\n",
      "n 2023년 11월 1~2일 영국 블레츨리 파크에서 열린 AI 안전성 정상회의(AI Safety Summit)에\n",
      "\n",
      "참가한 28개국 대표들이 AI 위험 관리를 위한 ‘블레츨리 선언’을 발표\n",
      "\n",
      "선언은 AI 안전 보장을 위해 국가, 국제기구, 기업, 시민사회, 학계를 포함한 모든 이해관계자의 협력이\n",
      "\n",
      "중요하다고 강조했으며, 특히 최첨단 AI 시스템 개발 기업은 안전 평가를 비롯한 적절한 조치를 취하여\n",
      "\n",
      "AI 시스템의 안전을 보장할 책임이 있다고 지적\n",
      "\n",
      "각국은 AI 안전 보장을 위해 첨단 AI 개발기업의 투명성 향상, 적절한 평가지표와 안전 테스트 도구\n",
      "\n",
      "개발, 공공부문 역량 구축과 과학 연구개발 등의 분야에서 협력하기로 합의\n",
      "\n",
      "£ 영국 총리, 정부 주도의 첨단 AI 시스템 안전 테스트 계획 발표\n",
      "\n",
      "n 리시 수낙 영국 총리는 AI 안전성 정상회의를 마무리하며 첨단 AI 모델에 대한 안전성 시험 계획\n",
      "\n",
      "수립과 테스트 수행을 주도할 영국 AI 안전 연구소의 출범을 발표\n",
      "\n",
      "첨단 AI 모델의 안전 테스트는 국가 안보와 안전, 사회적 피해를 포함한 여러 잠재적 유해 기능에 대한\n",
      "\n",
      "시험을 포함하며, 참석자들은 정부 주도의 외부 안전 테스트에 합의\n",
      "\n",
      "각국 정부는 테스트와 기타 안전 연구를 위한 공공부문 역량에 투자하고, 테스트 결과가 다른 국가와\n",
      "\n",
      "관련된 경우 해당 국가와 결과를 공유하며, 적절한 시기에 공동 표준 개발을 위해 노력하기로 합의\n",
      "\n",
      "n 참가국들은 튜링상을 수상한 AI 학자인 요슈아 벤지오 교수가 주도하는 ‘과학의 현황(State of\n",
      "\n",
      "the Science)’ 보고서 작성에도 합의했으며, 보고서를 통해 첨단 AI의 위험과 가능성에 관한\n"
     ]
    }
   ],
   "source": [
    "print(elements[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Pair 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Context information is below. You are only aware of this context and nothing else.\n",
    "---------------------\n",
    "\n",
    "{context}\n",
    "\n",
    "---------------------\n",
    "Given this context, generate only questions based on the below query.\n",
    "You are an Teacher/Professor in {domain}. \n",
    "Your task is to provide exactly **{num_questions}** question(s) for an upcoming quiz/examination. \n",
    "You are not to provide more or less than this number of questions. \n",
    "The question(s) should be diverse in nature across the document. \n",
    "The purpose of question(s) is to test the understanding of the students on the context information provided.\n",
    "You must also provide the answer to each question. The answer should be based on the context information provided only.\n",
    "\n",
    "Restrict the question(s) to the context information provided only.\n",
    "QUESTION and ANSWER should be written in Korean. response in JSON format which contains the `question` and `answer`.\n",
    "DO NOT USE List in JSON format.\n",
    "ANSWER should be a complete sentence.\n",
    "\n",
    "#Format:\n",
    "```json\n",
    "{{\n",
    "    \"QUESTION\": \"바이든 대통령이 서명한 '안전하고 신뢰할 수 있는 AI 개발과 사용에 관한 행정명령'의 주요 목적 중 하나는 무엇입니까?\",\n",
    "    \"ANSWER\": \"바이든 대통령이 서명한 행정명령의 주요 목적은 AI의 안전 마련과 보안 기준 마련을 위함입니다.\"\n",
    "}},\n",
    "{{\n",
    "    \"QUESTION\": \"메타의 라마2가 오픈소스 모델 중에서 어떤 유형의 작업에서 가장 우수한 성능을 발휘했습니까?\",\n",
    "    \"ANSWER\": \"메타의 라마2는 RAG 없는 질문과 답변 및 긴 형식의 텍스트 생성에서 오픈소스 모델 중 가장 우수한 성능을 발휘했습니다.\"    \n",
    "}},\n",
    "{{\n",
    "    \"QUESTION\": \"IDC 예측에 따르면 2027년까지 생성 AI 플랫폼과 애플리케이션 시장의 매출은 얼마로 전망되나요?\",\n",
    "    \"ANSWER\": \"IDC 예측에 따르면 2027년까지 생성 AI 플랫폼과 애플리케이션 시장의 매출은 283억 달러로 전망됩니다.\"    \n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'elements' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     11\u001b[39m chain = (\n\u001b[32m     12\u001b[39m     prompt\n\u001b[32m     13\u001b[39m     | ChatOpenAI(\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     | custom_json_parser\n\u001b[32m     20\u001b[39m )  \u001b[38;5;66;03m# 체인을 구성합니다.\u001b[39;00m\n\u001b[32m     22\u001b[39m qa_pair = []\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m \u001b[43melements\u001b[49m[\u001b[32m1\u001b[39m:]:\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m element.text:\n\u001b[32m     26\u001b[39m         qa_pair.extend(\n\u001b[32m     27\u001b[39m             chain.invoke(\n\u001b[32m     28\u001b[39m                 {\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m: element.text, \u001b[33m\"\u001b[39m\u001b[33mdomain\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mAI\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mnum_questions\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m3\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m     29\u001b[39m             )\n\u001b[32m     30\u001b[39m         )\n",
      "\u001b[31mNameError\u001b[39m: name 'elements' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "def custom_json_parser(response):\n",
    "    json_string = response.content.strip().removeprefix(\"```json\\n\").removesuffix(\"\\n```\").strip()\n",
    "    json_string = f'[{json_string}]'\n",
    "    return json.loads(json_string)\n",
    "\n",
    "chain = (\n",
    "    prompt\n",
    "    | ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0,\n",
    "        streaming=True,\n",
    "        callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    )\n",
    "    | custom_json_parser\n",
    ")  # 체인을 구성합니다.\n",
    "\n",
    "qa_pair = []\n",
    "\n",
    "for element in elements[1:]:\n",
    "    if element.text:\n",
    "        qa_pair.extend(\n",
    "            chain.invoke(\n",
    "                {\"context\": element.text, \"domain\": \"AI\", \"num_questions\": \"3\"}\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'QUESTION': '바이든 대통령이 2023년 10월 30일 발표한 행정명령의 주요 내용 중 하나는 무엇입니까?',\n",
       "  'ANSWER': '바이든 대통령이 발표한 행정명령의 주요 내용 중 하나는 AI의 안전과 보안 기준 마련입니다.'},\n",
       " {'QUESTION': \"G7이 2023년 10월 30일 합의한 '히로시마 AI 프로세스'의 주요 목적은 무엇입니까?\",\n",
       "  'ANSWER': \"G7이 합의한 '히로시마 AI 프로세스'의 주요 목적은 AI 기업을 대상으로 AI 위험 식별과 완화를 위한 국제 행동강령을 마련하는 것입니다.\"},\n",
       " {'QUESTION': '행정명령에 따르면 AI 시스템의 안전성과 신뢰성을 확인하기 위해 기업이 미국 정부와 공유해야 하는 정보는 무엇입니까?',\n",
       "  'ANSWER': '행정명령에 따르면 AI 시스템의 안전성과 신뢰성을 확인하기 위해 기업이 미국 정부와 공유해야 하는 정보는 안전 테스트 결과와 시스템에 관한 주요 정보입니다.'},\n",
       " {'QUESTION': 'G7 국가들이 채택한 AI 국제 행동강령의 주요 조치 중 하나는 무엇입니까?',\n",
       "  'ANSWER': 'G7 국가들이 채택한 AI 국제 행동강령의 주요 조치 중 하나는 첨단 AI 시스템의 성능과 한계를 공개하고 적절하거나 부적절한 사용영역을 알리는 방법으로 투명성을 보장하고 책임성을 강화하는 것입니다.'},\n",
       " {'QUESTION': \"영국 블레츨리 파크에서 개최된 AI 안전성 정상회의에서 발표된 '블레츨리 선언'의 주요 내용은 무엇입니까?\",\n",
       "  'ANSWER': \"'블레츨리 선언'의 주요 내용은 AI 안전 보장을 위해 국가, 국제기구, 기업, 시민사회, 학계를 포함한 모든 이해관계자의 협력이 중요하다는 점을 강조하고, 첨단 AI 시스템 개발 기업이 안전 평가를 비롯한 적절한 조치를 취하여 AI 시스템의 안전을 보장할 책임이 있다는 것입니다.\"},\n",
       " {'QUESTION': '영국 AI 안전 연구소가 주도할 예정인 첨단 AI 모델의 안전 테스트는 어떤 잠재적 유해 기능에 대한 시험을 포함합니까?',\n",
       "  'ANSWER': '영국 AI 안전 연구소가 주도할 예정인 첨단 AI 모델의 안전 테스트는 국가 안보와 안전, 사회적 피해를 포함한 여러 잠재적 유해 기능에 대한 시험을 포함합니다.'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디버깅을 위한 데이터셋 추가\n",
    "additional_qa = [\n",
    "    {\n",
    "        \"QUESTION\": \"테디노트 유튜브 채널에 대해서 알려주세요.\",\n",
    "        \"ANSWER\": \"테디노트(TeddyNote)는 데이터 분석, 머신러닝, 딥러닝 등의 주제를 다루는 유튜브 채널입니다. 이 채널을 운영하는 이경록님은 데이터 분석과 인공지능에 대한 다양한 강의를 제공하며, 초보자도 쉽게 따라할 수 있도록 친절하게 설명합니다.\",\n",
    "    },\n",
    "    {\n",
    "        \"QUESTION\": \"랭체인 관련 튜토리얼은 어디서 찾을 수 있나요?\",\n",
    "        \"ANSWER\": \"테디노트의 위키독스 페이지에는 LangChain에 대한 다양한 한국어 튜토리얼이 제공됩니다. 링크: https://wikidocs.net/book/14314\",\n",
    "    },\n",
    "    {\n",
    "        \"QUESTION\": \"테디노트 운영자에 대해서 알려주세요\",\n",
    "        \"ANSWER\": \"테디노트(TeddyNote) 운영자는 이경록(Teddy Lee)입니다. 그는 데이터 분석, 머신러닝, 딥러닝 분야에서 활동하는 전문가로, 다양한 교육 및 강의를 통해 지식을 공유하고 있습니다. 이경록님은 여러 기업과 교육기관에서 파이썬, 데이터 분석, 텐서플로우 등 다양한 주제로 강의를 진행해 왔습니다\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'QUESTION': '바이든 대통령이 2023년 10월 30일 발표한 행정명령의 주요 내용 중 하나는 무엇입니까?',\n",
       "  'ANSWER': '바이든 대통령이 발표한 행정명령의 주요 내용 중 하나는 AI의 안전과 보안 기준 마련입니다.'},\n",
       " {'QUESTION': 'G7 국가들이 채택한 AI 국제 행동강령의 주요 조치 중 하나는 무엇입니까?',\n",
       "  'ANSWER': 'G7 국가들이 채택한 AI 국제 행동강령의 주요 조치 중 하나는 첨단 AI 시스템의 성능과 한계를 공개하고 적절하거나 부적절한 사용영역을 알리는 방법으로 투명성을 보장하고 책임성을 강화하는 것입니다.'},\n",
       " {'QUESTION': '한국은 영국 정부와 몇 개월 뒤에 온라인으로 AI 미니 정상회의를 공동 개최하기로 합의했습니까?',\n",
       "  'ANSWER': '한국은 영국 정부와 6개월 뒤에 온라인으로 AI 미니 정상회의를 공동 개최하기로 합의했습니다.'},\n",
       " {'QUESTION': 'FTC가 생성 AI와 관련하여 주목하고 있는 주요 위험 요소는 무엇입니까?',\n",
       "  'ANSWER': 'FTC는 생성 AI와 관련하여 소비자의 개인정보 침해, 차별과 편견의 자동화, 사기 범죄 등의 위험에 주목하고 있습니다.'},\n",
       " {'QUESTION': '프런티어 모델 포럼이 AI 안전 연구를 위해 조성한 기금의 규모는 얼마입니까?',\n",
       "  'ANSWER': '프런티어 모델 포럼이 AI 안전 연구를 위해 조성한 기금의 규모는 1,000만 달러입니다.'},\n",
       " {'QUESTION': '데이터 출처 탐색기(Data Provenance Explorer) 플랫폼의 주요 기능은 무엇입니까?',\n",
       "  'ANSWER': '데이터 출처 탐색기 플랫폼의 주요 기능은 데이터셋의 라이선스 상태를 쉽게 파악하고, 주요 데이터셋의 구성과 데이터 계보도를 추적할 수 있게 하는 것입니다.'},\n",
       " {'QUESTION': \"삼성전자가 공개한 생성 AI 모델 '삼성 가우스'는 어떤 장점을 가지고 있습니까?\",\n",
       "  'ANSWER': '삼성 가우스는 온디바이스에서 작동 가능하여 외부로 사용자 정보가 유출될 위험이 없다는 장점을 가지고 있습니다.'},\n",
       " {'QUESTION': '앤스로픽은 구글의 클라우드 서비스 사용을 위해 얼마 규모의 계약을 체결했습니까?',\n",
       "  'ANSWER': '앤스로픽은 구글의 클라우드 서비스 사용을 위해 4년간 30억 달러 규모의 계약을 체결했습니다.'},\n",
       " {'QUESTION': '유튜브는 생성 AI 콘텐츠에 AI 라벨을 표시하지 않을 경우 어떤 조치를 취할 계획입니까?',\n",
       "  'ANSWER': '유튜브는 생성 AI 콘텐츠에 AI 라벨을 표시하지 않으면 해당 콘텐츠를 삭제하고 광고 수익을 배분하는 유튜브 파트너 프로그램도 정지할 수 있습니다.'},\n",
       " {'QUESTION': '구글 딥마인드 연구진이 제시한 범용 AI(AGI) 모델의 분류 체계에서, 현재 챗GPT와 구글 바드는 어떤 단계에 속합니까?',\n",
       "  'ANSWER': '구글 딥마인드 연구진이 제시한 범용 AI(AGI) 모델의 분류 체계에서, 현재 챗GPT와 구글 바드는 범용 AI 1단계 수준에 속합니다.'},\n",
       " {'QUESTION': \"GPT-4와 GPT-3.5 터보는 'LLM 환각 지수' 평가에서 어떤 성능을 보였습니까?\",\n",
       "  'ANSWER': 'GPT-4는 작업 유형과 관계없이 가장 우수한 성능을 보였으며, GPT-3.5 터보도 거의 동등한 성능을 발휘했습니다.'},\n",
       " {'QUESTION': 'CES 2024 전시회의 주제는 무엇입니까?',\n",
       "  'ANSWER': \"CES 2024 전시회의 주제는 '올 온(All on)'입니다.\"},\n",
       " {'QUESTION': '테디노트 유튜브 채널에 대해서 알려주세요.',\n",
       "  'ANSWER': '테디노트(TeddyNote)는 데이터 분석, 머신러닝, 딥러닝 등의 주제를 다루는 유튜브 채널입니다. 이 채널을 운영하는 이경록님은 데이터 분석과 인공지능에 대한 다양한 강의를 제공하며, 초보자도 쉽게 따라할 수 있도록 친절하게 설명합니다.'},\n",
       " {'QUESTION': '랭체인 관련 튜토리얼은 어디서 찾을 수 있나요?',\n",
       "  'ANSWER': '테디노트의 위키독스 페이지에는 LangChain에 대한 다양한 한국어 튜토리얼이 제공됩니다. 링크: https://wikidocs.net/book/14314'},\n",
       " {'QUESTION': '테디노트 운영자에 대해서 알려주세요',\n",
       "  'ANSWER': '테디노트(TeddyNote) 운영자는 이경록(Teddy Lee)입니다. 그는 데이터 분석, 머신러닝, 딥러닝 분야에서 활동하는 전문가로, 다양한 교육 및 강의를 통해 지식을 공유하고 있습니다. 이경록님은 여러 기업과 교육기관에서 파이썬, 데이터 분석, 텐서플로우 등 다양한 주제로 강의를 진행해 왔습니다'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_pair.extend(additional_qa)\n",
    "qa_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jsonl 파일로 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'QUESTION': '바이든 대통령이 2023년 10월 30일 발표한 행정명령의 주요 내용 중 하나는 무엇입니까?',\n",
       "  'ANSWER': '바이든 대통령이 발표한 행정명령의 주요 내용 중 하나는 AI의 안전과 보안 기준 마련입니다.'},\n",
       " {'QUESTION': 'G7 국가들이 채택한 AI 국제 행동강령의 주요 조치 중 하나는 무엇입니까?',\n",
       "  'ANSWER': 'G7 국가들이 채택한 AI 국제 행동강령의 주요 조치 중 하나는 첨단 AI 시스템의 성능과 한계를 공개하고 적절하거나 부적절한 사용영역을 알리는 방법으로 투명성을 보장하고 책임성을 강화하는 것입니다.'},\n",
       " {'QUESTION': '한국은 영국 정부와 몇 개월 뒤에 온라인으로 AI 미니 정상회의를 공동 개최하기로 합의했습니까?',\n",
       "  'ANSWER': '한국은 영국 정부와 6개월 뒤에 온라인으로 AI 미니 정상회의를 공동 개최하기로 합의했습니다.'},\n",
       " {'QUESTION': 'FTC가 생성 AI와 관련하여 주목하고 있는 주요 위험 요소는 무엇입니까?',\n",
       "  'ANSWER': 'FTC는 생성 AI와 관련하여 소비자의 개인정보 침해, 차별과 편견의 자동화, 사기 범죄 등의 위험에 주목하고 있습니다.'},\n",
       " {'QUESTION': '프런티어 모델 포럼이 AI 안전 연구를 위해 조성한 기금의 규모는 얼마입니까?',\n",
       "  'ANSWER': '프런티어 모델 포럼이 AI 안전 연구를 위해 조성한 기금의 규모는 1,000만 달러입니다.'},\n",
       " {'QUESTION': '데이터 출처 탐색기(Data Provenance Explorer) 플랫폼의 주요 기능은 무엇입니까?',\n",
       "  'ANSWER': '데이터 출처 탐색기 플랫폼의 주요 기능은 데이터셋의 라이선스 상태를 쉽게 파악하고, 주요 데이터셋의 구성과 데이터 계보도를 추적할 수 있게 하는 것입니다.'},\n",
       " {'QUESTION': \"삼성전자가 공개한 생성 AI 모델 '삼성 가우스'는 어떤 장점을 가지고 있습니까?\",\n",
       "  'ANSWER': '삼성 가우스는 온디바이스에서 작동 가능하여 외부로 사용자 정보가 유출될 위험이 없다는 장점을 가지고 있습니다.'},\n",
       " {'QUESTION': '앤스로픽은 구글의 클라우드 서비스 사용을 위해 얼마 규모의 계약을 체결했습니까?',\n",
       "  'ANSWER': '앤스로픽은 구글의 클라우드 서비스 사용을 위해 4년간 30억 달러 규모의 계약을 체결했습니다.'},\n",
       " {'QUESTION': '유튜브는 생성 AI 콘텐츠에 AI 라벨을 표시하지 않을 경우 어떤 조치를 취할 계획입니까?',\n",
       "  'ANSWER': '유튜브는 생성 AI 콘텐츠에 AI 라벨을 표시하지 않으면 해당 콘텐츠를 삭제하고 광고 수익을 배분하는 유튜브 파트너 프로그램도 정지할 수 있습니다.'},\n",
       " {'QUESTION': '구글 딥마인드 연구진이 제시한 범용 AI(AGI) 모델의 분류 체계에서, 현재 챗GPT와 구글 바드는 어떤 단계에 속합니까?',\n",
       "  'ANSWER': '구글 딥마인드 연구진이 제시한 범용 AI(AGI) 모델의 분류 체계에서, 현재 챗GPT와 구글 바드는 범용 AI 1단계 수준에 속합니다.'},\n",
       " {'QUESTION': \"GPT-4와 GPT-3.5 터보는 'LLM 환각 지수' 평가에서 어떤 성능을 보였습니까?\",\n",
       "  'ANSWER': 'GPT-4는 작업 유형과 관계없이 가장 우수한 성능을 보였으며, GPT-3.5 터보도 거의 동등한 성능을 발휘했습니다.'},\n",
       " {'QUESTION': 'CES 2024 전시회의 주제는 무엇입니까?',\n",
       "  'ANSWER': \"CES 2024 전시회의 주제는 '올 온(All on)'입니다.\"},\n",
       " {'QUESTION': '테디노트 유튜브 채널에 대해서 알려주세요.',\n",
       "  'ANSWER': '테디노트(TeddyNote)는 데이터 분석, 머신러닝, 딥러닝 등의 주제를 다루는 유튜브 채널입니다. 이 채널을 운영하는 이경록님은 데이터 분석과 인공지능에 대한 다양한 강의를 제공하며, 초보자도 쉽게 따라할 수 있도록 친절하게 설명합니다.'},\n",
       " {'QUESTION': '랭체인 관련 튜토리얼은 어디서 찾을 수 있나요?',\n",
       "  'ANSWER': '테디노트의 위키독스 페이지에는 LangChain에 대한 다양한 한국어 튜토리얼이 제공됩니다. 링크: https://wikidocs.net/book/14314'},\n",
       " {'QUESTION': '테디노트 운영자에 대해서 알려주세요',\n",
       "  'ANSWER': '테디노트(TeddyNote) 운영자는 이경록(Teddy Lee)입니다. 그는 데이터 분석, 머신러닝, 딥러닝 분야에서 활동하는 전문가로, 다양한 교육 및 강의를 통해 지식을 공유하고 있습니다. 이경록님은 여러 기업과 교육기관에서 파이썬, 데이터 분석, 텐서플로우 등 다양한 주제로 강의를 진행해 왔습니다'},\n",
       " {'QUESTION': '테디노트 유튜브 채널에 대해서 알려주세요.',\n",
       "  'ANSWER': '테디노트(TeddyNote)는 데이터 분석, 머신러닝, 딥러닝 등의 주제를 다루는 유튜브 채널입니다. 이 채널을 운영하는 이경록님은 데이터 분석과 인공지능에 대한 다양한 강의를 제공하며, 초보자도 쉽게 따라할 수 있도록 친절하게 설명합니다.'},\n",
       " {'QUESTION': '랭체인 관련 튜토리얼은 어디서 찾을 수 있나요?',\n",
       "  'ANSWER': '테디노트의 위키독스 페이지에는 LangChain에 대한 다양한 한국어 튜토리얼이 제공됩니다. 링크: https://wikidocs.net/book/14314'},\n",
       " {'QUESTION': '테디노트 운영자에 대해서 알려주세요',\n",
       "  'ANSWER': '테디노트(TeddyNote) 운영자는 이경록(Teddy Lee)입니다. 그는 데이터 분석, 머신러닝, 딥러닝 분야에서 활동하는 전문가로, 다양한 교육 및 강의를 통해 지식을 공유하고 있습니다. 이경록님은 여러 기업과 교육기관에서 파이썬, 데이터 분석, 텐서플로우 등 다양한 주제로 강의를 진행해 왔습니다'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/qa_pair.jsons\", \"w\") as f:\n",
    "    json.dumps(original_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'QUESTION': '여러분의 이름을 넣어보세요', 'ANSWER': '답변을 적어보세요...'},\n",
       " {'QUESTION': '랭체인 관련 튜토리얼은 어디서 찾을 수 있나요?',\n",
       "  'ANSWER': '테디노트의 위키독스 페이지에는 LangChain에 대한 다양한 한국어 튜토리얼이 제공됩니다. 링크: https://wikidocs.net/book/14314'},\n",
       " {'QUESTION': '테디노트 운영자에 대해서 알려주세요',\n",
       "  'ANSWER': '테디노트(TeddyNote) 운영자는 이경록(Teddy Lee)입니다. 그는 데이터 분석, 머신러닝, 딥러닝 분야에서 활동하는 전문가로, 다양한 교육 및 강의를 통해 지식을 공유하고 있습니다. 이경록님은 여러 기업과 교육기관에서 파이썬, 데이터 분석, 텐서플로우 등 다양한 주제로 강의를 진행해 왔습니다'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"qa_pair.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    original_qa = json.dumps(qa_pair, ensure_ascii=False)\n",
    "    \n",
    "# 디버깅을 위한 데이터셋 추가\n",
    "additional_qa = [\n",
    "    {\n",
    "        \"QUESTION\": \"여러분의 이름을 넣어보세요\",\n",
    "        \"ANSWER\": \"답변을 적어보세요...\",\n",
    "    },\n",
    "    {\n",
    "        \"QUESTION\": \"랭체인 관련 튜토리얼은 어디서 찾을 수 있나요?\",\n",
    "        \"ANSWER\": \"테디노트의 위키독스 페이지에는 LangChain에 대한 다양한 한국어 튜토리얼이 제공됩니다. 링크: https://wikidocs.net/book/14314\",\n",
    "    },\n",
    "    {\n",
    "        \"QUESTION\": \"테디노트 운영자에 대해서 알려주세요\",\n",
    "        \"ANSWER\": \"테디노트(TeddyNote) 운영자는 이경록(Teddy Lee)입니다. 그는 데이터 분석, 머신러닝, 딥러닝 분야에서 활동하는 전문가로, 다양한 교육 및 강의를 통해 지식을 공유하고 있습니다. 이경록님은 여러 기업과 교육기관에서 파이썬, 데이터 분석, 텐서플로우 등 다양한 주제로 강의를 진행해 왔습니다\",\n",
    "    },\n",
    "]\n",
    "\n",
    "original_qa = json.loads(original_qa)\n",
    "original_qa.extend(additional_qa)\n",
    "original_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"qa_pair.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for qa in original_qa:\n",
    "        f.write(json.dumps(qa, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"qa_pair.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for qa in qa_pair:\n",
    "        qa_modified = {\n",
    "            \"instruction\": qa[\"QUESTION\"],\n",
    "            \"input\": \"\",\n",
    "            \"output\": qa[\"ANSWER\"],\n",
    "        }\n",
    "        f.write(json.dumps(qa_modified, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFace datasets 라이브러리를 사용하여 데이터셋을 로드합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add97ab5403045d0b702d487faf974a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSchemaInferenceError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DOHYEON\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr--LeusF5K-py3.11\\Lib\\site-packages\\datasets\\builder.py:1850\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split_single\u001b[39m\u001b[34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[39m\n\u001b[32m   1849\u001b[39m num_shards = shard_id + \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1850\u001b[39m num_examples, num_bytes = \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1851\u001b[39m writer.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DOHYEON\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr--LeusF5K-py3.11\\Lib\\site-packages\\datasets\\arrow_writer.py:740\u001b[39m, in \u001b[36mArrowWriter.finalize\u001b[39m\u001b[34m(self, close_stream)\u001b[39m\n\u001b[32m    739\u001b[39m         \u001b[38;5;28mself\u001b[39m.stream.close()\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SchemaInferenceError(\u001b[33m\"\u001b[39m\u001b[33mPlease pass `features` or at least one example when writing data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    741\u001b[39m logger.debug(\n\u001b[32m    742\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDone writing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._num_examples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.unit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._num_bytes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m bytes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._path\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m._path\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    743\u001b[39m )\n",
      "\u001b[31mSchemaInferenceError\u001b[39m: Please pass `features` or at least one example when writing data",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDatasetGenerationError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m jsonl_file = \u001b[33m\"\u001b[39m\u001b[33mqa_pair.jsonl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# JSONL 파일을 Dataset으로 로드\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjsonl_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DOHYEON\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr--LeusF5K-py3.11\\Lib\\site-packages\\datasets\\load.py:1417\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance.as_streaming_dataset(split=split)\n\u001b[32m   1416\u001b[39m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1417\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1422\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1423\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1425\u001b[39m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[32m   1426\u001b[39m keep_in_memory = (\n\u001b[32m   1427\u001b[39m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance.info.dataset_size)\n\u001b[32m   1428\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DOHYEON\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr--LeusF5K-py3.11\\Lib\\site-packages\\datasets\\builder.py:897\u001b[39m, in \u001b[36mDatasetBuilder.download_and_prepare\u001b[39m\u001b[34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[39m\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    896\u001b[39m     prepare_split_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_proc\u001b[39m\u001b[33m\"\u001b[39m] = num_proc\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m.info.dataset_size = \u001b[38;5;28msum\u001b[39m(split.num_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info.splits.values())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DOHYEON\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr--LeusF5K-py3.11\\Lib\\site-packages\\datasets\\builder.py:973\u001b[39m, in \u001b[36mDatasetBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[39m\n\u001b[32m    969\u001b[39m split_dict.add(split_generator.split_info)\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    972\u001b[39m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    975\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    976\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot find data file. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    977\u001b[39m         + (\u001b[38;5;28mself\u001b[39m.manual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    978\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    979\u001b[39m         + \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[32m    980\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DOHYEON\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr--LeusF5K-py3.11\\Lib\\site-packages\\datasets\\builder.py:1705\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split\u001b[39m\u001b[34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[39m\n\u001b[32m   1703\u001b[39m job_id = \u001b[32m0\u001b[39m\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m1705\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_prepare_split_args\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DOHYEON\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr--LeusF5K-py3.11\\Lib\\site-packages\\datasets\\builder.py:1861\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split_single\u001b[39m\u001b[34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[39m\n\u001b[32m   1859\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[32m   1860\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1861\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[33m\"\u001b[39m\u001b[33mAn error occurred while generating the dataset\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1863\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\n",
      "\u001b[31mDatasetGenerationError\u001b[39m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# JSONL 파일 경로\n",
    "jsonl_file = \"qa_pair.jsonl\"\n",
    "\n",
    "# JSONL 파일을 Dataset으로 로드\n",
    "dataset = load_dataset(\"json\", data_files=jsonl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# HfApi 인스턴스 생성\n",
    "api = HfApi()\n",
    "\n",
    "# 데이터셋을 업로드할 리포지토리 이름\n",
    "repo_name = \"teddylee777/QA-Dataset-mini\"\n",
    "\n",
    "# 데이터셋을 허브에 푸시\n",
    "dataset.push_to_hub(repo_name, token=\"허깅페이스 토큰\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr--LeusF5K-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
